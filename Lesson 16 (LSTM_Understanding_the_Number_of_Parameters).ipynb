{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyMBPjRrDTN5oQzkrmKcPFOY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# **LSTM: Understanding the Number of Parameters**\n","In this tutorial, we will focus on internal structure of Keras LSTM layer in order to understand how many **learnable parameters** an LTSM layer has.\n","\n","Why do we **need** to care of calculating number of parameters in LSTM layer ***since*** we can easily get this number in the model summary report?\n","\n","Well there are several reasons:\n","* First of all, to calculate the number of learnable parameters correctly, we need to **understand how LSTM is structured** and how **LSTM operates** ***in depth***. Thus, we will delve into LSTM gates and gate functions. We will gain precious insight about how LSTM handles ***time dependent*** or ***sequence*** input data.\n","\n","* Secondly, in ANN models, number of parameter is a really important metric for understanding the **model capacity** and **complexity**. We need to keep an eye on the number of parameters of each layer in the model to handle ***overfitting*** or ***underfitting*** situations. One way to prevent these situations is to **adjust the number of parameters** of each layer. We need to know how number of parameters actullay affects the performance of each layer.\n","\n","If you want to enhance your understanding of LSTM layer and learn how many learnable parameters it has please continue this tutorial.\n","\n","By the way, I would like to mention that in [my Youtube channel](https://www.youtube.com/channel/UCrCxCxTFL2ytaDrDYrN4_eA) I have a dedicated playlist in English ([All About LSTM](https://www.youtube.com/playlist?list=PLQflnv_s49v_i1OVqE0DENBk-QJt9THjE)) and in Turkish  ([LSTM Hakkında Herşey](https://www.youtube.com/playlist?list=PLQflnv_s49v9rxWesjI1GVs8qfxwwHDR4)). You can check these playlist to learn more about LSTM.\n","\n","Lastly, if you want to be notified for upcoming tutorials about LSTM and Deep Learning **please subscribe to** [my Youtube channel](https://www.youtube.com/channel/UCrCxCxTFL2ytaDrDYrN4_eA) and a**ctivate notifications**.\n","\n","**Thank you!**\n","\n","\n","Now, let's get started!"],"metadata":{"id":"YfjZw5XTMDwD"}},{"cell_type":"markdown","source":["### **1. Prepare**"],"metadata":{"id":"54C6IwZl-zpA"}},{"cell_type":"markdown","source":["##### **1.1 Check GPU**"],"metadata":{"id":"aTYzvmMw_-i6"}},{"cell_type":"code","source":["%tensorflow_version 2.x\n","import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","  print('GPU device NOT found')\n","else:\n","  print('Found GPU at: {}'.format(device_name))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J9VJOKKXAD18","executionInfo":{"status":"ok","timestamp":1710262356828,"user_tz":-420,"elapsed":3590,"user":{"displayName":"Khánh Xuân","userId":"01744464682071429582"}},"outputId":"a8c420b0-ab4d-4e02-b9af-d20c482bc36f"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab only includes TensorFlow 2.x; %tensorflow_version has no effect.\n","Found GPU at: /device:GPU:0\n"]}]},{"cell_type":"markdown","source":["##### **1.2 Version info**"],"metadata":{"id":"C0ziaI-1CGMc"}},{"cell_type":"code","source":["import keras\n","print('tf version: ', tf.__version__)\n","print('tf.keras version: ', keras.__version__)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Y8TrAhVYBhS2","executionInfo":{"status":"ok","timestamp":1710262356829,"user_tz":-420,"elapsed":6,"user":{"displayName":"Khánh Xuân","userId":"01744464682071429582"}},"outputId":"84d7279e-c15b-437a-b17c-5b54dee03123"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["tf version:  2.15.0\n","tf.keras version:  2.15.0\n"]}]},{"cell_type":"markdown","source":["##### **1.3 Import Libraries**"],"metadata":{"id":"PUUoDMztFNtk"}},{"cell_type":"code","source":["from random import randint\n","from numpy import array\n","from numpy import argmax\n","import keras.backend as K\n","\n","from tensorflow.keras import Input\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.layers import LSTM, Bidirectional\n","\n","from tensorflow.keras.layers import Dense, Flatten\n","from tensorflow.keras.layers import TimeDistributed, RepeatVector"],"metadata":{"id":"cEVGenu_CtV2","executionInfo":{"status":"ok","timestamp":1710262796903,"user_tz":-420,"elapsed":363,"user":{"displayName":"Khánh Xuân","userId":"01744464682071429582"}}},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":["##### **References:**\n","[tf.keras.layers.LSTM official website](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)\n","\n","[Counting No. of Parameters in Deep Learning Models by Hand by Raimi Karim](https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889)\n","\n","[llustrated Guide to LSTM’s and GRU’s: A step by step explanation by\n","Michael Phi](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n","\n","[Animated RNN, LSTM and GRU by Raimi Karim\n","](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)\n","\n","[Number of parameters in Keras LSTM by  Dejan Batanjac](https://dejanbatanjac.github.io/2019/02/12/Number-of-parameters-in-Keras-LSTM.html#:~:text=The%20number%20of%20U%20parameters,parameters%20based%20on%20input%20shape.)"],"metadata":{"id":"SDrmRsZHBxaT"}},{"cell_type":"markdown","source":["### **2. REMINDER:**\n","\n","* LSTM ***expects*** **input data** to be a **3D** tensor such that:\n","\n","      [batch_size, timesteps, feature]\n","\n","* **batch_size:** how many samples in each batch during training and testing\n","\n","* **timesteps:** means **how many values exist in a sequence**. For example in [4, 7, 8, 4] there are 4 timesteps\n","\n","* **features:** how many **dimensions** are used **to represent a data in one time step**. For example, if each value in the sequence is one hot encoded with 9 zero and 1 one then feature is 10\n","* Example:\n","    \n","    In raw format:\n","\n","    X=[4, 7, 8, 4]\n","\n","    In one hot encoded format with 10 dimensions (feature = 10):\n","\n","    X=[[0 0 0 0 1 0 0 0 0 0]\n","\n","     [0 0 0 0 0 0 0 1 0 0]\n","    \n","    [0 0 0 0 0 0 0 0 1 0]\n","    \n","    [0 0 0 0 1 0 0 0 0 0]]"],"metadata":{"id":"L5mQf1xjIZ4d"}},{"cell_type":"markdown","source":["### **3. A Sample LSTM Model**\n","By the way, you can access [this Colab Notebook from the link](https://colab.research.google.com/drive/1Wd5340XLy-MC3YJPh3MkDnbGQ4JSnFwp?usp=sharing) in the video description"],"metadata":{"id":"U4U98zZ1I52k"}},{"cell_type":"code","source":["# Define Model\n","# 1 sequence có 40 samples, mỗi smaple có 3 feature\n","timesteps = 40          # dimensionality of the input sequence\n","features = 3            # dimensionality of each input representation in the sequence\n","LSTMoutputDimension = 2 # dimensionality of the LSTM outputs (Hidden & Cell states)\n","\n","input = Input(shape=(timesteps, features))\n","output = LSTM(LSTMoutputDimension)(input)\n","model_LSTM = Model(inputs=input, outputs=output)\n","\n","model_LSTM.summary()"],"metadata":{"id":"5McJXJagJAxr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710262797831,"user_tz":-420,"elapsed":563,"user":{"displayName":"Khánh Xuân","userId":"01744464682071429582"}},"outputId":"ec91aada-c2d0-4d7b-db60-18e3b34bb861"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 40, 3)]           0         \n","                                                                 \n"," lstm_2 (LSTM)               (None, 2)                 48        \n","                                                                 \n","=================================================================\n","Total params: 48 (192.00 Byte)\n","Trainable params: 48 (192.00 Byte)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["### **4. FINDING NUMBER OF PARAMETERS IN A NEURAL NETWORK WITH A SINGLE DENSE LAYER**\n","* Before explaining how to calculate number of LSTM parameters,  I would like to remind you how to calculate number of a dense layer's parameters.\n","*As we will see soon, LSTM has *4 dense layers* in its internal structure. So this discussion will help us a lot soon.\n","*Assume that\n","\n"," **i**= input size\n","\n","  **h**= size of hidden layer (number of neurons in the hidden layer)\n","\n","  **o**= output size (number of neurons in the output layer)\n","\n","* For a **single** hidden layer,\n","\n","  **number of parameters in the model** = connections between layers + biases in every layer (hiden + output layers!)\n","  \n","  = (i×h + h×o) + (h+o)\n","\n","* Check the below Figure [taken from here](https://towardsdatascience.com/counting-no-of-parameters-in-deep-learning-models-by-hand-8f1716241889)\n","\n","![link text](https://miro.medium.com/max/501/1*sTV2UIv76WQiHt4JfCqk9g.png)\n","\n","We can find the number of parameters by counting the number of connections between layers and by adding bias.\n","\n","* **connections** (weigths) between layers:\n","  *  between **input** and **hidden** layer is\n","\n","   i * h = 3 * 5 = 15\n","  * between **hidden** and **output** layer is\n","\n","   h * o = 5 * 2 = 10\n","* **biases** in every layer\n","  * biases in **hidden** layer\n","   \n","   h = 5\n","  * biases in **output** layer\n","\n","   o = 2\n","* **Total**:\n","\n","  15 + 10 + 5 + 2 = 32 parameters (weights + biases)   \n","\n","\n","Let's create a simple model and verify the calculation:\n"],"metadata":{"id":"5YnyNcA5mlql"}},{"cell_type":"code","source":["input = Input((None, 3))\n","dense = Dense(5)(input)\n","output = Dense(2)(dense)\n","model_dense = Model(input, output)\n","model_dense.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oKHVNW1soWzo","executionInfo":{"status":"ok","timestamp":1710263426884,"user_tz":-420,"elapsed":6,"user":{"displayName":"Khánh Xuân","userId":"01744464682071429582"}},"outputId":"f444aed2-52eb-40ec-f2fa-f49449c487ec"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_2 (InputLayer)        [(None, None, 3)]         0         \n","                                                                 \n"," dense (Dense)               (None, None, 5)           20        \n","                                                                 \n"," dense_1 (Dense)             (None, None, 2)           12        \n","                                                                 \n","=================================================================\n","Total params: 32 (128.00 Byte)\n","Trainable params: 32 (128.00 Byte)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["### **5. KERAS LSTM CELL STRUCTURE**\n","<img src=\"https://github.com/kmkarakaya/ML_tutorials/blob/master/images/LSTM_internal2.png?raw=true\" width=\"700\">\n","\n","[Image from here](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)"],"metadata":{"id":"E-Y9LEhwpNmz"}},{"cell_type":"markdown","source":["##### **5.1 FIRST EXPLANATION USING LSTM INTERNAL STRUCTURE**\n","Review the above Figure to capture the internal structure of LSTM cell\n","\n","* There are `3 inputs` to the LSTM cell:\n","  * $h_{t-1}$ previous timestep (t-1) Hidden State value\n","  * $c_{t-1}$ previous timestep (t-1) Cell State value\n","  * $x_{t}$   current timestep (t) Input value\n","\n","* There are `4 dens` layers:\n","  * Forget Gate\n","  * Input Gates = Input + Candidate\n","  * Output Gate\n","\n","* The input output tensor sizes (dimensions) are symbolized with circles. In the figure,\n","  * **Cell and Hidden states are vectors which have a dimension = 2.** This number is defined by the programmer by setting LSTM parameter units (LSTMoutputDimension) to 2\n","  * **Input is a vector which has a dimension = 3.** This number is also defined by the programmer by deciding how many dimension would be to represent an input (e.g. dimension of one-hot encoding, word embedding, etc.)  \n","\n","* Note that, By definition:\n","  * Hidden and Cell states vector dimensions must be the same\n","  * Hidden and Cell states vector dimensions at time **t-1** and **t** must be the same\n","  * Each input in the sequence must have the same vector dimensions\n"],"metadata":{"id":"1TgBqUAwpsP6"}},{"cell_type":"markdown","source":["##### **5.2 Let's focus on Forget Gate**\n","![link text](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/LSTM_forgetGate.png?raw=true)\n","\n","* As seen in above figure,\n","  * there are Hidden state values (**2  $h_{t-1}$** red circles) and\n","  * Input values ( **3 $x_{t}$** green circles)\n","* Total 5 numbers ( **2  $h_{t-1}$** + **3 $x_{t}$** ) are inputted to   **a dense layer**\n","* Output layer has **2  values** (which **must be equal** to the dimension of  $h_{t-1}$ Hidden state vector in the LSTM Cell)\n","\n","* We can calculate the number of parameters in this dense layer as we did before:\n","\n","  = ($h_{t-1}$ + $x_{t}$) × $h_{t-1}$ + $h_{t-1}$\n","\n","  = (2 + 3) × 2 + 2\n","\n","  = 12\n","\n","* **Thus Forget Gate has 12 parameters (weights + biases)**\n","\n","<img src=\"https://github.com/kmkarakaya/ML_tutorials/blob/master/images/LSTM_internal2.png?raw=true\" width=\"500\">\n","\n","[Image from here](https://towardsdatascience.com/animated-rnn-lstm-and-gru-ef124d06cf45)\n"],"metadata":{"id":"cuTwj8JiqA9Y"}},{"cell_type":"markdown","source":["* Since there are **4 gates** in the LSTM unit which have  exactly the **same dense layer** architecture,  there will be\n","\n","   = 4 × 12\n","\n","   **= 48 parameters**\n","\n","* We can formulate the parameter numbers in a LSTM layer given that $x$ is the input dimension, $h$ is the number of LSTM units / cells / latent space / output dimension:\n"],"metadata":{"id":"qKA-HETNqWVY"}},{"cell_type":"markdown","source":["##### **Note:**\n","* **LSTM parameter number = 4 × (($x$ + $h$) × $h$ +$h$)**"],"metadata":{"id":"N4MT2k8rqehu"}},{"cell_type":"markdown","source":["##### **5.3 SECOND EXPLANATION USING LSTM FUNCTION DEFINITIONS**\n","<img src=\"https://github.com/kmkarakaya/ML_tutorials/blob/master/images/LSTM_internal2.png?raw=true\" width=\"500\">\n","\n","\n","* The outputs of the 4 gates in the above figure can be expressed as a function as below:\n","\n","<img src=\"https://github.com/kmkarakaya/ML_tutorials/blob/master/images/LSTM_functions.png?raw=true\" width=\"300\">\n","\n","![link text](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/LSTM_functions2.png?raw=true)\n","\n","\n","\n","* As seen, among these functions, there are 4 functions which have\n","  * 2 weight matrices: $W$ and $U$ for $h_{t-1}$ and $x_{t}$ values\n","  * 1 bias vector $b$\n","\n","* We can work on **forget gate's function** $f_{t}$ to calculate parameter numbers:\n","\n","  ![link text](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/LSTM_forgetGate.png?raw=true)\n","\n"," ![link text](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/LSTM_functions3.png?raw=true)\n","\n"," Notice that we can guess the size (shape) of W,U and b given:\n"," * Input size ($h_{t-1}$ and $x_{t}$ )\n"," * Output size ($h_{t-1}$)\n","\n"," Since output must equal to Hidden State (hx1) size:\n","\n","  * for W param =  ($h$ × $x$)\n","  * for U param =  ($h$ × $h$)\n","  * for Biases  param =   $h$\n","\n"," * total params = W param + U param + Biases param\n","  \n","    =  ($h$ × $x$) +  ($h$ × $h$) +  $h$\n","\n","    =  ( ($h$ × $x$) +  ($h$ × $h$) +   $h$ )\n","\n","    =  ( ($x$ + $h$) ×  $h$  +   $h$ )\n","\n","* there are 4 functions which are exactly defined in the same way, in the LSTM layer, there will be\n"],"metadata":{"id":"XC0w5HVUqkGF"}},{"cell_type":"markdown","source":["##### **Note:**\n","* **LSTM parameter number = 4 × (($x$ + $h$) × $h$ +$h$)**"],"metadata":{"id":"iR7sfuO4rEaw"}},{"cell_type":"markdown","source":["### **6. Example**"],"metadata":{"id":"wnKkmgZwtanG"}},{"cell_type":"code","source":["# Define Model\n","# 1 sequence có 40 samples, mỗi smaple có 3 feature\n","timesteps = 40          # dimensionality of the input sequence\n","features = 3            # dimensionality of each input representation in the sequence\n","LSTMoutputDimension = 2 # dimensionality of the LSTM outputs (Hidden & Cell states)\n","\n","input = Input(shape=(timesteps, features))\n","output = LSTM(LSTMoutputDimension)(input)\n","model_LSTM = Model(inputs=input, outputs=output)\n","\n","model_LSTM.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j1_lyplztdMR","executionInfo":{"status":"ok","timestamp":1710264575363,"user_tz":-420,"elapsed":603,"user":{"displayName":"Khánh Xuân","userId":"01744464682071429582"}},"outputId":"d3a1ff4e-481c-4613-9596-d04c2d0bb26d"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"model_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_3 (InputLayer)        [(None, 40, 3)]           0         \n","                                                                 \n"," lstm_3 (LSTM)               (None, 2)                 48        \n","                                                                 \n","=================================================================\n","Total params: 48 (192.00 Byte)\n","Trainable params: 48 (192.00 Byte)\n","Non-trainable params: 0 (0.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","source":["In the above model,\n","* LSTM layer has `dimensionality of the output space (unit) parameter value 2` which means `that Hidden and Cell states are vectors with dimension 2`\n","* input for each time step is represented by a vector with dimension 3 (feature)\n","\n","Remember:\n","\n","**LSTM parameter number = 4 × (($x$ + $h$) × $h$ + $h$)**\n","\n","**LSTM parameter number = 4 × ((3 + 2) × 2 + 2)**\n","\n","**LSTM parameter number = 4 × (12)**\n","\n","**LSTM parameter number = 48**\n","\n","\n","Summary indicates the total number parameters of the model (actually LSTM layer) is **48** as we computed above!\n","\n","Now, let's get sizes of each weight matrix and bias vector from the model:\n","\n"],"metadata":{"id":"3Tjti-lBtl4y"}},{"cell_type":"code","source":["W = model_LSTM.layers[1].get_weights()[0]\n","U = model_LSTM.layers[1].get_weights()[1]\n","b = model_LSTM.layers[1].get_weights()[2]\n","\n","print(\"W\", W.size, ' calculated as 4*features*LSTMoutputDimension ',\n","      4*features*LSTMoutputDimension)\n","print(\"U\", U.size, ' calculated as 4*LSTMoutputDimension*LSTMoutputDimension ',\n","      4*LSTMoutputDimension*LSTMoutputDimension)\n","print(\"b\", b.size , ' calculated as 4*LSTMoutputDimension ',\n","      4*LSTMoutputDimension)\n","print(\"Total Parameter Number: W+ U + b \" ,\n","      W.size+ U.size + b.size)\n","print(\"Total Parameter Number: 4 × ((x + h) × h +h) \" ,\n","      4* ((features+LSTMoutputDimension)*LSTMoutputDimension+LSTMoutputDimension))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c-GzA_GLt-Fj","executionInfo":{"status":"ok","timestamp":1710266445556,"user_tz":-420,"elapsed":403,"user":{"displayName":"Khánh Xuân","userId":"01744464682071429582"}},"outputId":"c699f23c-7cb6-473c-a3cb-afddb320df53"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["W 24  calculated as 4*features*LSTMoutputDimension  24\n","U 16  calculated as 4*LSTMoutputDimension*LSTMoutputDimension  16\n","b 8  calculated as 4*LSTMoutputDimension  8\n","Total Parameter Number: W+ U + b  48\n","Total Parameter Number: 4 × ((x + h) × h +h)  48\n"]}]},{"cell_type":"markdown","source":["### **IMPORTANT**:\n","* **timesteps** (or ***input_length***) of the input sequence does **NOT** affect the number of parameters!\n","  \n","  **WHY?**\n","  \n","  Because,  same \"W\", \"U\", and \"b\" are **shared** ***throughout the time-steps***\n","\n","  That is; instead of using new weights and biases at each time step, LSTM unit **uses the same** \"W\", \"U\", and \"b\" values for ***all time-steps***!\n","\n","  This **simplifies** the calculation of backpropagation and **reduce** the number of parameters (memory requirement)"],"metadata":{"id":"zj9nhEwE0usn"}},{"cell_type":"markdown","source":["### **7. CONCLUSION**\n","* In this tutorial, we investigate the internal structure of Keras LSTM layer to calculate the number of learnable parameters.\n","* We examine several concepts: time steps, dimentionality of the output space, gates, gate functions etc.\n","* We come up with the correct formulation in 2 different ways:\n","  \n","  **LSTM parameter number = 4 × (($x$ + $h$) × $h$ + $h$)**\n","* We check the correctness of the formulation by creating a simple LSTM model\n","\n","From here, you can continue learning about LSTM for example with understanding the outputs of LSTM layer. I have a tutorial for it already on [Youtube](https://youtu.be/B66760rvHA8) and [Medium](https://medium.com/@kmkarakaya/lstm-understanding-output-types-e93d2fb57c77).\n","\n","Enjoy!"],"metadata":{"id":"8GyNo9o22RAO"}},{"cell_type":"markdown","source":["### **8. BONUS: Shapes of Matrices and Vectors:**"],"metadata":{"id":"-qim7adr2ecn"}},{"cell_type":"code","source":["# define model\n","timesteps=40\n","features=3\n","LSTMoutputDimension = 2\n","\n","input = Input(shape=(timesteps, features))\n","output= LSTM(LSTMoutputDimension)(input)\n","model_LSTM = Model(inputs=input, outputs=output)\n","\n","W = model_LSTM.layers[1].get_weights()[0]\n","U = model_LSTM.layers[1].get_weights()[1]\n","b = model_LSTM.layers[1].get_weights()[2]"],"metadata":{"id":"Ucl2hQ8o2ffC","executionInfo":{"status":"ok","timestamp":1710266997552,"user_tz":-420,"elapsed":391,"user":{"displayName":"Khánh Xuân","userId":"01744464682071429582"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["print(\"Shapes of Matrices and Vecors:\")\n","print(\"Input [batch_size, timesteps, feature] \", input.shape)\n","print(\"Input feature/dimension (x in formulations)\", input.shape[2])\n","print(\"Number of Hidden States/LSTM units (cells)/dimensionality of the output space (h in formulations)\", LSTMoutputDimension)\n","print(\"W\", W.shape)\n","print(\"U\", U.shape)\n","print(\"b\", b.shape)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mZU2t7TF2iRK","executionInfo":{"status":"ok","timestamp":1710267000132,"user_tz":-420,"elapsed":679,"user":{"displayName":"Khánh Xuân","userId":"01744464682071429582"}},"outputId":"59b1cee1-7fef-4e0a-84cd-8fe3662dcaed"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Shapes of Matrices and Vecors:\n","Input [batch_size, timesteps, feature]  (None, 40, 3)\n","Input feature/dimension (x in formulations) 3\n","Number of Hidden States/LSTM units (cells)/dimensionality of the output space (h in formulations) 2\n","W (3, 8)\n","U (2, 8)\n","b (8,)\n"]}]},{"cell_type":"markdown","source":["**Why 8?**\n","* Remember the functions? Examine Forget gate function below:\n","\n"," ![link text](https://github.com/kmkarakaya/ML_tutorials/blob/master/images/LSTM_functions3.png?raw=true)\n","\n","* W matrix multiplies input vector (x) whose dimension in the example is 1x3 to produce results as many as hidden states which is 2 in the example.\n","* Thus, it is expected that W dimension should be **3x2** **but why 3x8?**  \n","* Because **there are 4 gates/functions!** and W is the matrix for ***the whole layer***!\n","* Therefore, W dimension is 3 x (2 x 4) = 3 x 8!\n","* Above fact also explains why U and b has 8 values\n","\n"," = 4 * number of hidden states!\n","\n","* We can formulate the dimensions of parameters as:\n","\n"," W [ x , 4 * h ]\n","\n"," U [ h , 4 * h ]\n","\n"," b [  4 * h ]\n","\n"," where  \n","    \n","    **x** is the number of dimension/feature of the input\n","\n","    **h** is the number of Hidden States/LSTM units (cells)/dimensionality of the output space"],"metadata":{"id":"LtgpnGvL2zDr"}}]}